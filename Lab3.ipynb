{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/ds-kiel/TinyML-Labs/blob/WS24-25/Lab3/Lab3.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/ds-kiel/TinyML-Labs/blob/WS24-25/Lab3/Lab3.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://raw.githubusercontent.com/ds-kiel/TinyML-Labs/WS24-25/Lab3/Lab3.ipynb\" download><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "Before starting, you must click on the \"Copy To Drive\" option in the top bar. Go to File --> Save a Copy to Drive. Name it *'Group\\<Your group number\\>_Lab1.ipynb'*. <ins>This is the master notebook so you will not be able to save your changes without copying it !</ins> Once you click on that, make sure you are working on that version of the notebook so that your work is saved.\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Pruning and pruning with quantization\n",
    "\n",
    "In the past labs, you looked at a full TinyML pipeline using time-series data. In this lab, we leave time-series data behind and look at a standard image dataset (CIFAR10). You will explore how to [prune](https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras) a network, and how to further improve space by quantizing the pruned model. **For running this lab, please use Python 3.11.**\n",
    "\n",
    "If you like to, you can also deploy your pruned model on the microcontroller ([Arduino Nano 33 BLE Sense](https://store.arduino.cc/products/arduino-tiny-machine-learning-kit)) and test its performance there.\n",
    "\n",
    "## Dataset: CIFAR10\n",
    "\n",
    "The CIFAR10 dataset, is a labeled subset of the [80 million tiny images](http://people.csail.mit.edu/torralba/tinyimages/) dataset by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The CIFAR-10 dataset consists of 60000 colour images with a resolution of 32x32 pixels (input shape: (32, 32, 3)) in 10 classes, with 6000 images per class. The dataset contains 50.000 training images and 10.000 test images.\n",
    "\n",
    "Here's an example of how the data looks:\n",
    "\n",
    "![Example of CIFAR10 images](https://www.tensorflow.org/static/tutorials/images/cnn_files/output_K3PAELE2eSU9_0.png)\n",
    "\n",
    "\n",
    "Each image in the dataset is assigned to one of the following labels:\n",
    "\n",
    "\n",
    "| Label | Description |\n",
    "| --- | --- |\n",
    "| 0 | airplane |\n",
    "| 1 | automobile |\n",
    "| 2 | bird |\n",
    "| 3 | cat |\n",
    "| 4 | deer |\n",
    "| 5 | dog |\n",
    "| 6 | frog |\n",
    "| 7 | horse |\n",
    "| 8 | ship |\n",
    "| 9 | truck |\n",
    "\n",
    "You can load and split the data with\n",
    "\n",
    "```python\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have not done so already, install the following dependencies\n",
    "!python -m pip install tensorflow==2.14 tf-keras tensorflow-model-optimization scikit-learn edgeimpulse numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# from tensorflow.lite import TFLiteConverter\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import edgeimpulse as ei\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "def plot_training_history(history, model_name):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.suptitle(f'Model {model_name}')\n",
    "    fig.set_figwidth(15)\n",
    "\n",
    "    ax1.plot(range(1, len(history.history['accuracy'])+1), history.history['accuracy'])\n",
    "    ax1.plot(range(1, len(history.history['val_accuracy'])+1), history.history['val_accuracy'])\n",
    "    ax1.set_title('Model accuracy')\n",
    "    ax1.set(xlabel='epoch', ylabel='accuracy')\n",
    "    ax1.legend(['training', 'validation'], loc='best')\n",
    "\n",
    "    ax2.plot(range(1, len(history.history['loss'])+1), history.history['loss'])\n",
    "    ax2.plot(range(1, len(history.history['val_loss'])+1), history.history['val_loss'])\n",
    "    ax2.set_title('Model loss')\n",
    "    ax2.set(xlabel='epoch', ylabel='loss')\n",
    "    ax2.legend(['training', 'validation'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(model, x_test, y_test, labels):\n",
    "    cm = confusion_matrix(y_test, np.argmax(model.predict(x_test),axis=1))\n",
    "\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    cm = pd.DataFrame(cm, index = labels,\n",
    "                    columns = labels)\n",
    "\n",
    "    plt.figure(figsize = (4,4))\n",
    "    ax = sns.heatmap(cm*100,\n",
    "            annot=True,\n",
    "            fmt='.1f',\n",
    "            cmap=\"Blues\",\n",
    "            cbar=False,\n",
    "                )\n",
    "    ax.set_ylabel(\"True Class\", fontdict= {'fontweight':'bold'})\n",
    "    ax.set_xlabel(\"Predicted Class\", fontdict= {'fontweight':'bold'})\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gzipped_model_size(file):\n",
    "  # Returns size of gzipped model, in bytes.\n",
    "  import os\n",
    "  import zipfile\n",
    "\n",
    "  _, zipped_file = tempfile.mkstemp('.zip')\n",
    "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "    f.write(file)\n",
    "\n",
    "  return os.path.getsize(zipped_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(interpreter, x_test, y_test):\n",
    "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "    # Run predictions on every image in the \"test\" dataset.\n",
    "    prediction_results = []\n",
    "    for i, test_image in enumerate(x_test):\n",
    "        if i % 1000 == 0:\n",
    "            print('Evaluated on {n} results so far.'.format(n=i))\n",
    "      \n",
    "        # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "        # the model's input data format.\n",
    "        test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "        interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "        # Run inference.\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Post-processing: remove batch dimension and find the class with highest\n",
    "        # probability.\n",
    "        output = interpreter.tensor(output_index)\n",
    "        img_class = np.argmax(output()[0])\n",
    "        prediction_results.append(img_class)\n",
    "\n",
    "    print('\\n')\n",
    "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "    prediction_results = np.array(prediction_results)\n",
    "    accuracy = (prediction_results == np.squeeze(y_test)).mean()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base model training\n",
    "\n",
    "So far, you manually explored how many epochs are necessary to successfully train the model. However, Tensorflow gives you an option to automate this called [early stopping](https://keras.io/api/callbacks/early_stopping/). See also [here](https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/) and [here](https://towardsdatascience.com/a-practical-introduction-to-early-stopping-in-machine-learning-550ac88bc8fd).\n",
    "\n",
    "---\n",
    "**Task 7:** Use an early stopping callback in your fitting function to find the optimal number of epochs. Use reasonable configurations. How many epochs does it train for?\n",
    "\n",
    "**Answer:** ...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model / data parameters\n",
    "labels = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "num_classes = len(labels)\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "# Load the data and split it between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model\n",
    "\n",
    "---\n",
    "**Task 1:** Build an MCU-sized model for the dataset that reaches at least an accuracy of 70%. You can start with the given model below but feel free to optimive it. For the model you choose, please discuss the number of parameters it uses and why it should fit on a microcontroller.\n",
    "\n",
    "**Answer:** ...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CIFAR10_model(summary=False):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "        keras.layers.MaxPooling2D((2, 2)),\n",
    "        keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        keras.layers.MaxPooling2D((2, 2)),\n",
    "        keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        keras.layers.MaxPooling2D((2, 2)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    model.build()\n",
    "\n",
    "    if summary:\n",
    "        model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "---\n",
    "**Task 2:** Train your neural network and use an [early stopping](https://keras.io/api/callbacks/early_stopping/) callback in your fitting function to find the optimal number of epochs. Use reasonable configurations. How many epochs does it train for?\n",
    "\n",
    "**Answer:** ...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = EarlyStopping(\n",
    "    monitor=...,\n",
    "    patience=...,\n",
    "    min_delta=...,\n",
    "    restore_best_weights=True,\n",
    "    mode=...\n",
    ")\n",
    "\n",
    "num_epochs = 200\n",
    "model = CIFAR10_model()\n",
    "history = model.fit(x_train, y_train, batch_size=128, epochs=num_epochs, validation_split=0.1, callbacks=[early_stopping_cb])\n",
    "plot_training_history(history, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, baseline_model_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "\n",
    "plot_confusion_matrix(model, x_test, y_test, labels)\n",
    "\n",
    "_, keras_file = tempfile.mkstemp('.h5')\n",
    "keras.models.save_model(model, keras_file, include_optimizer=False)\n",
    "print('Saved baseline model to:', keras_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization Aware Training (for comparison)\n",
    "\n",
    "---\n",
    "**Task 3:** Perform quantization aware training for comparison.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "q_aware_model = quantize_model(model)\n",
    "q_aware_model.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_es = q_aware_model.fit(x_train, y_train, batch_size=128,\n",
    "                              epochs=num_epochs, validation_split=0.1, callbacks=[early_stopping_cb])\n",
    "plot_training_history(history_es, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, q_aware_model_accuracy = q_aware_model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Quantization aware test accuracy:', q_aware_model_accuracy)\n",
    "\n",
    "plot_confusion_matrix(q_aware_model, x_test, y_test, labels)\n",
    "\n",
    "_, q_aware_file = tempfile.mkstemp('.h5')\n",
    "keras.models.save_model(q_aware_model, q_aware_file, include_optimizer=False)\n",
    "print('Saved baseline model to:', q_aware_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the model to LiteRT and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_litert_model = converter.convert()\n",
    "\n",
    "_, quantized_litert_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(quantized_litert_file, 'wb') as f:\n",
    "  f.write(quantized_litert_model)\n",
    "\n",
    "print('Saved quantized and pruned TFLite model to:', quantized_litert_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning\n",
    "\n",
    "---\n",
    "**Task 4:** Create a model for [pruning](https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras#fine-tune_pre-trained_model_with_pruning) and explain it. Feel free to change the code and try other things. Also have a look at this [guide](https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide).\n",
    "\n",
    "**Answer:** ...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "# Compute end step to finish pruning after ... epochs.\n",
    "batch_size = 128\n",
    "epochs = ...\n",
    "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
    "\n",
    "num_images = x_train.shape[0] * (1 - validation_split)\n",
    "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "# Define model for pruning.\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=...\n",
    "                                                               final_sparsity=...\n",
    "                                                               begin_step=0,\n",
    "                                                               end_step=end_step)\n",
    "}\n",
    "\n",
    "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "model_for_pruning.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**Task 5:** Look at some of the weights of the network.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_pruning.weights[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**Task 6:** Train the model and explain the callbacks used.\n",
    "\n",
    "**Answer:** ...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = tempfile.mkdtemp()\n",
    "\n",
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "]\n",
    "\n",
    "model_for_pruning.fit(x_train, y_train,\n",
    "                  batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
    "                  callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**Task 7:** Compare the weights of the same layer to the state before pruning. What changed?\n",
    "\n",
    "**Answer:** ...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_pruning.weights[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and save the pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
    "   x_test, y_test, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy) \n",
    "print('Pruned test accuracy:', model_for_pruning_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the pruned model for export\n",
    "\n",
    "---\n",
    "**Task 8:** Prepare the pruned model for export. How did the number of parameters change?\n",
    "\n",
    "**Answer:** ...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "model_for_export.summary()\n",
    "\n",
    "_, pruned_keras_file = tempfile.mkstemp('.h5')\n",
    "keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
    "print('Saved pruned Keras model to:', pruned_keras_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**Task 9:** Convert the pruned model to LiteRT.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "pruned_litert_model = converter.convert()\n",
    "\n",
    "_, pruned_litert_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(pruned_litert_file, 'wb') as f:\n",
    "  f.write(pruned_litert_model)\n",
    "\n",
    "print('Saved pruned TFLite model to:', pruned_litert_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**Task 10:** Quantize the pruned model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_and_pruned_litert_model = converter.convert()\n",
    "\n",
    "_, quantized_and_pruned_litert_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(quantized_and_pruned_litert_file, 'wb') as f:\n",
    "  f.write(quantized_and_pruned_litert_model)\n",
    "\n",
    "print('Saved quantized and pruned TFLite model to:', quantized_and_pruned_litert_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "---\n",
    "**Task 10:** Compare the size of the models and create a plot for comparison.\n",
    "\n",
    "**Task 11:** Compare the performance of the models and create a plot for comparison.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
    "print(\"Size of gzipped quantized Keras model: %.2f bytes\" % (get_gzipped_model_size(q_aware_file)))\n",
    "print(\"Size of gzipped quantized LiteRT model: %.2f bytes\" % (get_gzipped_model_size(quantized_litert_file)))\n",
    "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n",
    "print(\"Size of gzipped pruned LiteRT model: %.2f bytes\" % (get_gzipped_model_size(pruned_litert_file)))\n",
    "print(\"Size of gzipped pruned and quantized LiteRT model: %.2f bytes\" % (get_gzipped_model_size(quantized_and_pruned_litert_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=quantized_litert_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "quantized_test_accuracy = evaluate_model(interpreter, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=quantized_and_pruned_litert_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "pruned_quantized_test_accuracy = evaluate_model(interpreter, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of baseline Keras model:', baseline_model_accuracy)\n",
    "print('Accuracy of quantized Keras model:', q_aware_model_accuracy)\n",
    "print('Accuracy of pruned Keras model:', model_for_pruning_accuracy)\n",
    "print('Accuracy of quantized LiteRT model:', quantized_test_accuracy)\n",
    "print('Accuracy of pruned and quantized LiteRT model:', pruned_quantized_test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) On-device execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**Task 12 (Optional):** Convert the pruned and quantized LiteRT model to a LiteRT for microcontroller program.\n",
    "\n",
    "**Task 13 (Optional):** Write an Arduino program that predicts the class of a CIFAR10 image transferred to the Arduino through the serial interface.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmllab3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
